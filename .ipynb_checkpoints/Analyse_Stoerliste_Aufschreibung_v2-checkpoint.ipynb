{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be11b931",
   "metadata": {},
   "source": [
    "# Analyse Störliste – Blatt „Aufschreibung“\n",
    "\n",
    "Dieses Notebook:\n",
    "1. lädt die Excel-Datei,\n",
    "2. entfernt alle **leeren Zeilen**, die **ab der Spalte „Dauer Org-Mangel“** (und alle folgenden Spalten) **keine Daten** enthalten,\n",
    "3. bereitet Zeit-/Dauerfelder auf und\n",
    "4. analysiert **Stoßzeiten**, **Maschinen/Stationen** und **Fehlerursachen** inkl. Auffälligkeiten bei der **Ausfalldauer**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datei-Pfad (im gleichen Ordner wie das Notebook oder anpassen)\n",
    "FILE_PATH = \"Stoerliste_Heckrungenanlage_2023_NEU.xlsx\"\n",
    "SHEET_NAME = \"Aufschreibung\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba7891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Laden ---\n",
    "df_raw = pd.read_excel(FILE_PATH, sheet_name=SHEET_NAME)\n",
    "\n",
    "print(\"Rohdaten:\", df_raw.shape)\n",
    "display(df_raw.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc573424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Spaltennamen normalisieren (Zeilenumbrüche/Mehrfachspaces entfernen) ---\n",
    "df = df_raw.copy()\n",
    "df.columns = [re.sub(r\"\\s+\", \" \", str(c).strip()) for c in df.columns]\n",
    "\n",
    "# Zielspalte finden (robust, falls in Excel Zeilenumbrüche/Spaces anders sind)\n",
    "target_pattern = re.compile(r\"^Dauer\\s*Org-?Mangel$\", re.IGNORECASE)\n",
    "start_col = None\n",
    "for c in df.columns:\n",
    "    if target_pattern.match(c):\n",
    "        start_col = c\n",
    "        break\n",
    "\n",
    "if start_col is None:\n",
    "    # Fallback: suche nach beiden Wörtern\n",
    "    candidates = [c for c in df.columns if (\"Dauer\" in c) and (\"Org\" in c) and (\"Mangel\" in c)]\n",
    "    if candidates:\n",
    "        start_col = candidates[0]\n",
    "\n",
    "if start_col is None:\n",
    "    raise ValueError(\"Spalte 'Dauer Org-Mangel' konnte nicht gefunden werden. Bitte Spaltennamen prüfen.\")\n",
    "\n",
    "start_idx = list(df.columns).index(start_col)\n",
    "cols_from = list(df.columns)[start_idx:]\n",
    "\n",
    "print(\"Startspalte:\", start_col)\n",
    "print(\"Spalten ab Startspalte:\", cols_from)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e1bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Leere Zeilen entfernen: wenn ab Startspalte (inkl.) ALLES leer ist ---\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Leere Strings -> NA (nur in object/string-Spalten)\n",
    "for c in cols_from:\n",
    "    if df_clean[c].dtype == object:\n",
    "        df_clean[c] = df_clean[c].astype(\"string\").str.strip()\n",
    "        df_clean.loc[df_clean[c].isin([\"\", \"nan\", \"NaN\"]), c] = pd.NA\n",
    "\n",
    "mask_keep = df_clean[cols_from].notna().any(axis=1)\n",
    "df_clean = df_clean.loc[mask_keep].copy()\n",
    "\n",
    "print(\"Nach dem Entfernen leerer Zeilen:\", df_clean.shape)\n",
    "display(df_clean.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e116b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: bereinigte Daten speichern\n",
    "df_clean.to_csv(\"Aufschreibung_clean.csv\", index=False)\n",
    "df_clean.to_excel(\"Aufschreibung_clean.xlsx\", index=False)\n",
    "\n",
    "print(\"Gespeichert als: Aufschreibung_clean.csv / Aufschreibung_clean.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e77081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aufbereitung: Zeitspalten, Ausfalldauer (min) ---\n",
    "# Dauer-Spalten (in Minuten) – ggf. anpassen, falls andere Namen vorkommen\n",
    "duration_cols = [\n",
    "    \"Dauer Org-Mangel\",\n",
    "    \"Dauer Anlagen-Ausfall\",\n",
    "    \"Dauer Anlagen-Ausfall intern\",\n",
    "    \"Dauer Logistik- Defizite\",\n",
    "]\n",
    "for c in duration_cols:\n",
    "    if c in df_clean.columns:\n",
    "        df_clean[c] = pd.to_numeric(df_clean[c], errors=\"coerce\")\n",
    "\n",
    "df_clean[\"Downtime_min\"] = df_clean[duration_cols].sum(axis=1, skipna=True)\n",
    "\n",
    "# Zeit robust in Sekunden umwandeln (Zeit-Objekte oder Strings)\n",
    "import datetime as dt\n",
    "def safe_time_to_seconds(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, dt.time):\n",
    "        return x.hour*3600 + x.minute*60 + x.second\n",
    "    s = str(x).strip()\n",
    "    # 'HH:MM' oder 'HH:MM:SS'\n",
    "    if re.match(r\"^\\d{1,2}:\\d{2}(:\\d{2})?$\", s):\n",
    "        parts = s.split(\":\")\n",
    "        h = int(parts[0]); m = int(parts[1]); sec = int(parts[2]) if len(parts) > 2 else 0\n",
    "        return h*3600 + m*60 + sec\n",
    "    # Excel kann Zeiten als Tagesbruchteil speichern\n",
    "    if re.match(r\"^\\d+(\\.\\d+)?$\", s):\n",
    "        frac = float(s)\n",
    "        return int(round(frac*24*3600))\n",
    "    return np.nan\n",
    "\n",
    "date_norm = pd.to_datetime(df_clean[\"Datum\"], errors=\"coerce\").dt.normalize()\n",
    "\n",
    "start_seconds = df_clean[\"Zeit von\"].apply(safe_time_to_seconds)\n",
    "end_seconds   = df_clean[\"Zeit bis\"].apply(safe_time_to_seconds)\n",
    "\n",
    "df_clean[\"Start\"] = date_norm + pd.to_timedelta(start_seconds, unit=\"s\")\n",
    "df_clean[\"End\"]   = date_norm + pd.to_timedelta(end_seconds, unit=\"s\")\n",
    "df_clean.loc[df_clean[\"End\"] < df_clean[\"Start\"], \"End\"] += pd.Timedelta(days=1)\n",
    "\n",
    "# Outage-Events: alle Zeilen mit Downtime > 0\n",
    "df_out = df_clean[df_clean[\"Downtime_min\"] > 0].copy()\n",
    "\n",
    "print(\"Events (Downtime>0):\", df_out.shape[0])\n",
    "print(\"Zeitraum:\", df_out[\"Datum\"].min(), \"bis\", df_out[\"Datum\"].max())\n",
    "df_out[[\"Datum\",\"Schicht\",\"Zeit von\",\"Zeit bis\",\"Downtime_min\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa84ce4",
   "metadata": {},
   "source": [
    "## 1) Überblick / KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi = {\n",
    "    \"Events (Downtime>0)\": int(df_out.shape[0]),\n",
    "    \"Gesamte Downtime (min)\": float(df_out[\"Downtime_min\"].sum()),\n",
    "    \"Ø Downtime je Event (min)\": float(df_out[\"Downtime_min\"].mean()),\n",
    "    \"Median Downtime (min)\": float(df_out[\"Downtime_min\"].median()),\n",
    "}\n",
    "pd.DataFrame([kpi])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea079403",
   "metadata": {},
   "source": [
    "## 2) Stoßzeiten (wann passieren Ausfälle?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17dc689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out[\"Start_hour\"] = df_out[\"Start\"].dt.hour\n",
    "hour_stats = (df_out.dropna(subset=[\"Start_hour\"])\n",
    "              .groupby(\"Start_hour\")\n",
    "              .agg(events=(\"Downtime_min\",\"size\"),\n",
    "                   downtime_min=(\"Downtime_min\",\"sum\"),\n",
    "                   avg_downtime=(\"Downtime_min\",\"mean\"))\n",
    "              .reset_index()\n",
    "              .sort_values(\"Start_hour\"))\n",
    "\n",
    "display(hour_stats)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(hour_stats[\"Start_hour\"], hour_stats[\"events\"])\n",
    "plt.title(\"Anzahl Ausfälle nach Start-Stunde\")\n",
    "plt.xlabel(\"Stunde (0-23)\")\n",
    "plt.ylabel(\"Anzahl Events\")\n",
    "plt.xticks(range(0,24,1))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(hour_stats[\"Start_hour\"], hour_stats[\"downtime_min\"])\n",
    "plt.title(\"Gesamte Downtime (min) nach Start-Stunde\")\n",
    "plt.xlabel(\"Stunde (0-23)\")\n",
    "plt.ylabel(\"Downtime (min)\")\n",
    "plt.xticks(range(0,24,1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Wochentag x Stunde (Event-Anzahl)\n",
    "df_out[\"DayName\"] = df_out[\"Datum\"].dt.day_name()\n",
    "order = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "\n",
    "heat = (df_out.pivot_table(index=\"DayName\", columns=\"Start_hour\", values=\"Downtime_min\",\n",
    "                           aggfunc=\"size\", fill_value=0)\n",
    "        .reindex(order))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.imshow(heat.values, aspect=\"auto\")\n",
    "plt.title(\"Heatmap: Anzahl Ausfälle (Wochentag x Stunde)\")\n",
    "plt.yticks(range(len(heat.index)), heat.index)\n",
    "plt.xticks(range(0,24,1), range(0,24,1))\n",
    "plt.xlabel(\"Stunde\")\n",
    "plt.colorbar(label=\"Anzahl Events\")\n",
    "plt.show()\n",
    "\n",
    "display(heat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8de97d",
   "metadata": {},
   "source": [
    "## 3) Welche Maschinen/Stationen haben die meisten Fehler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed04a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out[\"Station_norm\"] = (df_out[\"Station/ OP\"].astype(\"string\")\n",
    "                          .str.upper()\n",
    "                          .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "                          .str.strip())\n",
    "\n",
    "station_stats = (df_out.groupby(\"Station_norm\")\n",
    "                 .agg(events=(\"Downtime_min\",\"size\"),\n",
    "                      downtime_min=(\"Downtime_min\",\"sum\"),\n",
    "                      avg_downtime=(\"Downtime_min\",\"mean\"))\n",
    "                 .sort_values([\"events\",\"downtime_min\"], ascending=False))\n",
    "\n",
    "display(station_stats.head(20))\n",
    "\n",
    "top10 = station_stats.head(10).reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(top10[\"Station_norm\"], top10[\"events\"])\n",
    "plt.title(\"Top 10 Stationen nach Anzahl Events\")\n",
    "plt.xlabel(\"Station\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(top10[\"Station_norm\"], top10[\"downtime_min\"])\n",
    "plt.title(\"Top 10 Stationen nach gesamter Downtime\")\n",
    "plt.xlabel(\"Station\")\n",
    "plt.ylabel(\"Downtime (min)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685876f2",
   "metadata": {},
   "source": [
    "## 4) Welche Fehler/Ursachen wie oft? (Unterbrechungsursache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_stats = (df_out.groupby(\"Unterbrechungsursache\")\n",
    "               .agg(events=(\"Downtime_min\",\"size\"),\n",
    "                    total_downtime=(\"Downtime_min\",\"sum\"),\n",
    "                    avg_downtime=(\"Downtime_min\",\"mean\"),\n",
    "                    median_downtime=(\"Downtime_min\",\"median\"))\n",
    "               .sort_values(\"events\", ascending=False))\n",
    "\n",
    "display(cause_stats.head(20))\n",
    "\n",
    "top_causes = cause_stats.head(12).reset_index()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(top_causes[\"Unterbrechungsursache\"].astype(str), top_causes[\"events\"])\n",
    "plt.title(\"Top Ursachen nach Häufigkeit\")\n",
    "plt.xlabel(\"Unterbrechungsursache\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.xticks(rotation=60, ha=\"right\")\n",
    "plt.show()\n",
    "\n",
    "top_causes_downtime = cause_stats.sort_values(\"total_downtime\", ascending=False).head(12).reset_index()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(top_causes_downtime[\"Unterbrechungsursache\"].astype(str), top_causes_downtime[\"total_downtime\"])\n",
    "plt.title(\"Top Ursachen nach gesamter Downtime\")\n",
    "plt.xlabel(\"Unterbrechungsursache\")\n",
    "plt.ylabel(\"Downtime (min)\")\n",
    "plt.xticks(rotation=60, ha=\"right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be261e",
   "metadata": {},
   "source": [
    "## 5) Auffälligkeiten bei der Ausfalldauer (Kombinationen, Ausreißer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung der Downtime\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(df_out[\"Downtime_min\"].dropna(), bins=30)\n",
    "plt.title(\"Verteilung: Downtime je Event (min)\")\n",
    "plt.xlabel(\"Downtime (min)\")\n",
    "plt.ylabel(\"Anzahl Events\")\n",
    "plt.show()\n",
    "\n",
    "# Downtime nach Schicht (Boxplot)\n",
    "plt.figure(figsize=(8,4))\n",
    "data = [df_out.loc[df_out[\"Schicht\"]==s, \"Downtime_min\"].dropna() for s in sorted(df_out[\"Schicht\"].dropna().unique())]\n",
    "labels = [s for s in sorted(df_out[\"Schicht\"].dropna().unique())]\n",
    "plt.boxplot(data, labels=labels, showfliers=False)\n",
    "plt.title(\"Downtime je Event nach Schicht (ohne Ausreißer)\")\n",
    "plt.xlabel(\"Schicht\")\n",
    "plt.ylabel(\"Downtime (min)\")\n",
    "plt.show()\n",
    "\n",
    "# Ausreißer: Top 20 längste Events\n",
    "top_long = (df_out.sort_values(\"Downtime_min\", ascending=False)\n",
    "            .loc[:, [\"Datum\",\"Start\",\"End\",\"Schicht\",\"Station_norm\",\"Unterbrechungsursache\",\"Downtime_min\",\"Bemerkung\"]]\n",
    "            .head(20))\n",
    "display(top_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1244b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammenhang mit Output / Personal (wenn vorhanden)\n",
    "num_cols = [\"Downtime_min\", \"Dauer Arbeits-zeit\", \"Anzahl MA\", \"Menge N.i. O.\", \"Profi\", \"AHV\", \"Menge Gesamt (Stück)\"]\n",
    "corr = df_out[num_cols].corr(method=\"spearman\", numeric_only=True)\n",
    "display(corr)\n",
    "\n",
    "# Scatter: Downtime vs Menge Gesamt\n",
    "plt.figure(figsize=(6,4))\n",
    "x = df_out[\"Menge Gesamt (Stück)\"]\n",
    "y = df_out[\"Downtime_min\"]\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.title(\"Downtime vs. Menge Gesamt (Stück)\")\n",
    "plt.xlabel(\"Menge Gesamt (Stück)\")\n",
    "plt.ylabel(\"Downtime (min)\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter: Downtime vs Anzahl MA\n",
    "plt.figure(figsize=(6,4))\n",
    "x = df_out[\"Anzahl MA\"]\n",
    "y = df_out[\"Downtime_min\"]\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.title(\"Downtime vs. Anzahl MA\")\n",
    "plt.xlabel(\"Anzahl MA\")\n",
    "plt.ylabel(\"Downtime (min)\")\n",
    "plt.show()\n",
    "\n",
    "# Aggregation nach Anzahl MA\n",
    "ma_stats = (df_out.groupby(\"Anzahl MA\")\n",
    "            .agg(events=(\"Downtime_min\",\"size\"),\n",
    "                 avg_downtime=(\"Downtime_min\",\"mean\"),\n",
    "                 total_downtime=(\"Downtime_min\",\"sum\"))\n",
    "            .sort_index())\n",
    "display(ma_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1ce5b",
   "metadata": {},
   "source": [
    "## 6) Ideen für nächste Schritte\n",
    "\n",
    "- **Freitext reduzieren:** „Freitext“ ist sehr häufig – ideal wäre eine standardisierte Fehlerklassifikation (Dropdown).\n",
    "- **Stationen konsolidieren:** z.B. `R 06` vs. `R06` (falls vorhanden) vereinheitlichen.\n",
    "- **Ausfälle nach Priorität:** Fokus auf Kombination aus **hoher Downtime** + **hoher Häufigkeit** (Pareto).\n",
    "- **Geplante vs. ungeplante Stops:** Ursachen wie „Wartungsplan“/„Reinigung“ ggf. separat betrachten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554593f5",
   "metadata": {},
   "source": [
    "## 7) Zusammenhänge: Schicht × Zeit × MA × Downtime × Ursache\n",
    "\n",
    "Die folgenden Blöcke zeigen dir die wichtigsten Kombinationen aus **Schicht**, **Uhrzeit**, **Anzahl MA**, **Ausfalldauer** und **Ursache** – als Tabellen und Visualisierungen.\n",
    "\n",
    "> Tipp: Wenn du sehr viele Ursachen hast, nutze die *Top-N*-Filter in den Zellen, damit die Plots lesbar bleiben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40302e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Engineering für Zusammenhangsanalysen ---\n",
    "df = df_clean.copy()\n",
    "\n",
    "# Robustheit / Standardisierung\n",
    "df[\"Ursache\"] = df.get(\"Unterbrechungsursache\", pd.Series(index=df.index)).fillna(\"Unbekannt\").astype(str).str.strip()\n",
    "df[\"Station\"] = df.get(\"Station/ OP\", pd.Series(index=df.index)).fillna(\"Unbekannt\").astype(str).str.strip()\n",
    "\n",
    "# Schicht-Label (Codes aus deinen Daten: f/s/n/t)\n",
    "shift_map = {\"f\": \"Früh\", \"s\": \"Spät\", \"n\": \"Nacht\", \"t\": \"Tag\"}\n",
    "df[\"Schicht_label\"] = df.get(\"Schicht\", pd.Series(index=df.index)).map(shift_map)\n",
    "df[\"Schicht_label\"] = df[\"Schicht_label\"].fillna(df.get(\"Schicht\", pd.Series(index=df.index)).fillna(\"Unbekannt\").astype(str))\n",
    "\n",
    "# Zeit-Features\n",
    "df[\"Start_ts\"] = pd.to_datetime(df.get(\"Start\", pd.Series(index=df.index)), errors=\"coerce\")\n",
    "df[\"Start_date\"] = df[\"Start_ts\"].dt.date\n",
    "df[\"Start_hour\"] = df[\"Start_ts\"].dt.hour\n",
    "\n",
    "weekday_de = {0:\"Mo\", 1:\"Di\", 2:\"Mi\", 3:\"Do\", 4:\"Fr\", 5:\"Sa\", 6:\"So\"}\n",
    "df[\"Wochentag_de\"] = df[\"Start_ts\"].dt.dayofweek.map(weekday_de)\n",
    "\n",
    "# Anzahl MA (numerisch) + Bins\n",
    "df[\"MA\"] = pd.to_numeric(df.get(\"Anzahl MA\", pd.Series(index=df.index)), errors=\"coerce\")\n",
    "df[\"MA_bin\"] = pd.cut(\n",
    "    df[\"MA\"],\n",
    "    bins=[-0.1, 2, 3, 4, 5, 6, 99],\n",
    "    labels=[\"<=2\", \"3\", \"4\", \"5\", \"6\", \">=7\"]\n",
    ")\n",
    "\n",
    "# Downtime (numerisch) + Bins\n",
    "df[\"Downtime_min\"] = pd.to_numeric(df.get(\"Downtime_min\", pd.Series(index=df.index)), errors=\"coerce\")\n",
    "df[\"Downtime_bin\"] = pd.cut(\n",
    "    df[\"Downtime_min\"],\n",
    "    bins=[-0.1, 0, 5, 15, 30, 60, 999999],\n",
    "    labels=[\"0\", \"1-5\", \"6-15\", \"16-30\", \"31-60\", \">60\"]\n",
    ")\n",
    "\n",
    "# Time-Blocks (optional)\n",
    "df[\"Tagesblock\"] = pd.cut(\n",
    "    df[\"Start_hour\"],\n",
    "    bins=[-0.1, 5, 11, 17, 23],\n",
    "    labels=[\"Nacht(0-5)\", \"Vormittag(6-11)\", \"Nachmittag(12-17)\", \"Abend(18-23)\"]\n",
    ")\n",
    "\n",
    "# Quick-Checks\n",
    "print(\"Zeilen:\", len(df))\n",
    "print(\"Schichten:\", df[\"Schicht_label\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Top Ursachen:\", df[\"Ursache\"].value_counts().head(10).to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4275aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hilfsfunktionen (Heatmap + Top-Kombinationen) ---\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def plot_heatmap(pivot: pd.DataFrame, title: str, xlabel: str = \"\", ylabel: str = \"\", annotate: bool = False):\n",
    "    # pivot: index=y, columns=x, values=z (numeric)\n",
    "    if pivot.empty:\n",
    "        print(\"Pivot ist leer – nichts zu plotten.\")\n",
    "        return\n",
    "\n",
    "    data = pivot.values.astype(float)\n",
    "    fig, ax = plt.subplots(figsize=(max(8, 0.45*len(pivot.columns)), max(3, 0.45*len(pivot.index))))\n",
    "    im = ax.imshow(data, aspect=\"auto\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(pivot.columns)))\n",
    "    ax.set_xticklabels(pivot.columns, rotation=90)\n",
    "    ax.set_yticks(np.arange(len(pivot.index)))\n",
    "    ax.set_yticklabels(pivot.index)\n",
    "\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    if annotate and data.size <= 600:  # nur kleine Heatmaps annotieren\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(data.shape[1]):\n",
    "                v = data[i, j]\n",
    "                if np.isfinite(v):\n",
    "                    ax.text(j, i, f\"{v:.0f}\", ha=\"center\", va=\"center\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def top_combos(df_in: pd.DataFrame, group_cols, n=20, sort_by=\"downtime_sum\"):\n",
    "    g = (df_in\n",
    "         .groupby(group_cols, dropna=False)\n",
    "         .agg(events=(\"Downtime_min\", \"size\"),\n",
    "              downtime_sum=(\"Downtime_min\", \"sum\"),\n",
    "              downtime_avg=(\"Downtime_min\", \"mean\"),\n",
    "              downtime_median=(\"Downtime_min\", \"median\"))\n",
    "         .reset_index()\n",
    "        )\n",
    "    return g.sort_values(sort_by, ascending=False).head(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d960bfca",
   "metadata": {},
   "source": [
    "### 7.1 Schicht × Uhrzeit (Stoßzeiten je Schicht)\n",
    "\n",
    "- **Heatmap Events** zeigt, wann in welcher Schicht die meisten Ereignisse starten.\n",
    "- **Heatmap Downtime-Summe** zeigt, wann die teuersten Zeitfenster sind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc79394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Schicht × Stunde: Events & Downtime ---\n",
    "pivot_events = pd.pivot_table(\n",
    "    df, index=\"Schicht_label\", columns=\"Start_hour\",\n",
    "    values=\"Downtime_min\", aggfunc=\"size\", fill_value=0\n",
    ").reindex(columns=list(range(24)), fill_value=0)\n",
    "\n",
    "pivot_downtime = pd.pivot_table(\n",
    "    df, index=\"Schicht_label\", columns=\"Start_hour\",\n",
    "    values=\"Downtime_min\", aggfunc=\"sum\", fill_value=0\n",
    ").reindex(columns=list(range(24)), fill_value=0)\n",
    "\n",
    "plot_heatmap(pivot_events, \"Events nach Schicht und Start-Stunde\", xlabel=\"Stunde\", ylabel=\"Schicht\", annotate=False)\n",
    "plot_heatmap(pivot_downtime, \"Downtime-Summe (min) nach Schicht und Start-Stunde\", xlabel=\"Stunde\", ylabel=\"Schicht\", annotate=False)\n",
    "\n",
    "# Top-Stunden je Schicht (nach Events)\n",
    "pivot_events.apply(lambda s: s.sort_values(ascending=False).head(5), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b83d4ef",
   "metadata": {},
   "source": [
    "### 7.2 Ursache × Schicht (welche Ursachen dominieren in welcher Schicht?)\n",
    "\n",
    "- **Top-N Ursachen** filtern, damit die Darstellung übersichtlich bleibt.\n",
    "- Tabellen zeigen **Events**, **Downtime-Summe** und **Ø-Downtime**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ursache × Schicht (Top-N) ---\n",
    "TOP_N = 12  # anpassen\n",
    "top_causes = df[\"Ursache\"].value_counts().head(TOP_N).index\n",
    "df_top = df[df[\"Ursache\"].isin(top_causes)].copy()\n",
    "\n",
    "tbl_shift_cause = (df_top\n",
    "    .groupby([\"Schicht_label\", \"Ursache\"], dropna=False)\n",
    "    .agg(events=(\"Downtime_min\",\"size\"),\n",
    "         downtime_sum=(\"Downtime_min\",\"sum\"),\n",
    "         downtime_avg=(\"Downtime_min\",\"mean\"))\n",
    "    .reset_index()\n",
    "    .sort_values([\"Schicht_label\",\"events\"], ascending=[True,False])\n",
    ")\n",
    "display(tbl_shift_cause)\n",
    "\n",
    "pivot_sc_events = tbl_shift_cause.pivot(index=\"Schicht_label\", columns=\"Ursache\", values=\"events\").fillna(0)\n",
    "pivot_sc_sum   = tbl_shift_cause.pivot(index=\"Schicht_label\", columns=\"Ursache\", values=\"downtime_sum\").fillna(0)\n",
    "pivot_sc_avg   = tbl_shift_cause.pivot(index=\"Schicht_label\", columns=\"Ursache\", values=\"downtime_avg\").fillna(0)\n",
    "\n",
    "plot_heatmap(pivot_sc_events, f\"Events: Schicht × Ursache (Top {TOP_N})\", xlabel=\"Ursache\", ylabel=\"Schicht\", annotate=False)\n",
    "plot_heatmap(pivot_sc_sum,   f\"Downtime-Summe: Schicht × Ursache (Top {TOP_N})\", xlabel=\"Ursache\", ylabel=\"Schicht\", annotate=False)\n",
    "plot_heatmap(pivot_sc_avg,   f\"Ø Downtime: Schicht × Ursache (Top {TOP_N})\", xlabel=\"Ursache\", ylabel=\"Schicht\", annotate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9956967",
   "metadata": {},
   "source": [
    "### 7.3 Ursache × Uhrzeit (wann tritt welche Ursache auf?)\n",
    "\n",
    "Hier siehst du, ob bestimmte Ursachen **zu bestimmten Uhrzeiten** gehäuft auftreten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ursache × Stunde (Top-N nach Häufigkeit) ---\n",
    "TOP_N = 12  # anpassen\n",
    "top_causes = df[\"Ursache\"].value_counts().head(TOP_N).index\n",
    "df_top = df[df[\"Ursache\"].isin(top_causes)].copy()\n",
    "\n",
    "pivot_cause_hour_cnt = pd.pivot_table(\n",
    "    df_top, index=\"Ursache\", columns=\"Start_hour\",\n",
    "    values=\"Downtime_min\", aggfunc=\"size\", fill_value=0\n",
    ").reindex(columns=list(range(24)), fill_value=0)\n",
    "\n",
    "pivot_cause_hour_avg = pd.pivot_table(\n",
    "    df_top, index=\"Ursache\", columns=\"Start_hour\",\n",
    "    values=\"Downtime_min\", aggfunc=\"mean\", fill_value=0\n",
    ").reindex(columns=list(range(24)), fill_value=np.nan)\n",
    "\n",
    "plot_heatmap(pivot_cause_hour_cnt, f\"Events: Ursache × Start-Stunde (Top {TOP_N})\", xlabel=\"Stunde\", ylabel=\"Ursache\", annotate=False)\n",
    "plot_heatmap(pivot_cause_hour_avg, f\"Ø Downtime: Ursache × Start-Stunde (Top {TOP_N})\", xlabel=\"Stunde\", ylabel=\"Ursache\", annotate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3b8df",
   "metadata": {},
   "source": [
    "### 7.4 Anzahl MA × Downtime (und Interaktion mit Schicht/Ursache)\n",
    "\n",
    "Diese Blöcke zeigen:\n",
    "- ob mehr/weniger Personal mit **längerer/kürzerer Downtime** zusammenhängt,\n",
    "- ob der Zusammenhang **je Schicht** oder **je Ursache** unterschiedlich ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c60276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MA × Downtime: Überblick ---\n",
    "tmp = df.dropna(subset=[\"MA\", \"Downtime_min\"]).copy()\n",
    "\n",
    "# Korrelationen (robust: Spearman)\n",
    "spearman = tmp[[\"MA\",\"Downtime_min\"]].corr(method=\"spearman\").iloc[0,1]\n",
    "pearson  = tmp[[\"MA\",\"Downtime_min\"]].corr(method=\"pearson\").iloc[0,1]\n",
    "print(\"Korrelation MA ↔ Downtime (Spearman):\", round(float(spearman), 3))\n",
    "print(\"Korrelation MA ↔ Downtime (Pearson): \", round(float(pearson), 3))\n",
    "\n",
    "# Scatter (MA vs Downtime)\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.scatter(tmp[\"MA\"], tmp[\"Downtime_min\"], alpha=0.25)\n",
    "plt.title(\"Scatter: Anzahl MA vs Downtime (min)\")\n",
    "plt.xlabel(\"Anzahl MA\")\n",
    "plt.ylabel(\"Downtime (min)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gruppiert: Schicht × MA (Events / Ø / Summe)\n",
    "tbl_shift_ma = (tmp\n",
    "    .groupby([\"Schicht_label\",\"MA\"], dropna=False)\n",
    "    .agg(events=(\"Downtime_min\",\"size\"),\n",
    "         downtime_sum=(\"Downtime_min\",\"sum\"),\n",
    "         downtime_avg=(\"Downtime_min\",\"mean\"))\n",
    "    .reset_index()\n",
    "    .sort_values([\"Schicht_label\",\"MA\"])\n",
    ")\n",
    "display(tbl_shift_ma)\n",
    "\n",
    "# Plot: Ø Downtime je MA und Schicht\n",
    "for sh in sorted(tbl_shift_ma[\"Schicht_label\"].dropna().unique()):\n",
    "    sub = tbl_shift_ma[tbl_shift_ma[\"Schicht_label\"]==sh]\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(sub[\"MA\"], sub[\"downtime_avg\"], marker=\"o\")\n",
    "    plt.title(f\"Ø Downtime je Anzahl MA – Schicht: {sh}\")\n",
    "    plt.xlabel(\"Anzahl MA\")\n",
    "    plt.ylabel(\"Ø Downtime (min)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MA × Downtime × Ursache (Top-N Ursachen) ---\n",
    "TOP_N = 10\n",
    "top_causes = df[\"Ursache\"].value_counts().head(TOP_N).index\n",
    "tmp = df[df[\"Ursache\"].isin(top_causes)].dropna(subset=[\"MA\",\"Downtime_min\"]).copy()\n",
    "\n",
    "tbl_cause_ma = (tmp\n",
    "    .groupby([\"Ursache\",\"MA\"], dropna=False)\n",
    "    .agg(events=(\"Downtime_min\",\"size\"),\n",
    "         downtime_sum=(\"Downtime_min\",\"sum\"),\n",
    "         downtime_avg=(\"Downtime_min\",\"mean\"))\n",
    "    .reset_index()\n",
    "    .sort_values([\"Ursache\",\"MA\"])\n",
    ")\n",
    "display(tbl_cause_ma)\n",
    "\n",
    "pivot = tbl_cause_ma.pivot(index=\"Ursache\", columns=\"MA\", values=\"downtime_avg\").fillna(0)\n",
    "plot_heatmap(pivot, f\"Ø Downtime: Ursache × Anzahl MA (Top {TOP_N})\", xlabel=\"Anzahl MA\", ylabel=\"Ursache\", annotate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ffeaf2",
   "metadata": {},
   "source": [
    "### 7.5 Dreifach-Kombinationen (Schicht × Station × Ursache)\n",
    "\n",
    "Damit findest du die **teuersten** und **häufigsten** Problemkombinationen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Top Kombinationen nach Downtime-Summe ---\n",
    "top_by_sum = top_combos(df, [\"Schicht_label\", \"Station\", \"Ursache\"], n=25, sort_by=\"downtime_sum\")\n",
    "display(top_by_sum)\n",
    "\n",
    "# --- Top Kombinationen nach Häufigkeit (Events) ---\n",
    "top_by_events = top_combos(df, [\"Schicht_label\", \"Station\", \"Ursache\"], n=25, sort_by=\"events\")\n",
    "display(top_by_events)\n",
    "\n",
    "# Optional: nur ungeplante Ursachen (simple Heuristik – anpassen!)\n",
    "planned_patterns = r\"(wartung|reinigung|geplant|plan|service)\"\n",
    "df_unplanned = df[~df[\"Ursache\"].str.contains(planned_patterns, case=False, na=False)].copy()\n",
    "\n",
    "top_unplanned_sum = top_combos(df_unplanned, [\"Schicht_label\",\"Station\",\"Ursache\"], n=25, sort_by=\"downtime_sum\")\n",
    "display(top_unplanned_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a438a",
   "metadata": {},
   "source": [
    "### 7.6 Auffällige lange Ausfälle: wer/wann/warum?\n",
    "\n",
    "- Zeigt die **Top-Langläufer** und deren Kombinationen.\n",
    "- Hilft, Ausreißer zu verstehen (z.B. 60-min-Blockungen, Wiederholungen, Sonderfälle).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Long-Stop Outlier Analyse ---\n",
    "q95 = df[\"Downtime_min\"].quantile(0.95)\n",
    "q99 = df[\"Downtime_min\"].quantile(0.99)\n",
    "print(\"P95 Downtime:\", q95, \"min\")\n",
    "print(\"P99 Downtime:\", q99, \"min\")\n",
    "\n",
    "long_df = df[df[\"Downtime_min\"] >= q95].copy()\n",
    "print(\"Anzahl Events >= P95:\", len(long_df))\n",
    "\n",
    "top_long = long_df.sort_values(\"Downtime_min\", ascending=False).head(30)[\n",
    "    [\"Start_ts\",\"Schicht_label\",\"Start_hour\",\"MA\",\"Station\",\"Ursache\",\"Downtime_min\",\"Bemerkung\"]\n",
    "]\n",
    "display(top_long)\n",
    "\n",
    "display(long_df[\"Schicht_label\"].value_counts())\n",
    "display(long_df[\"Ursache\"].value_counts().head(15))\n",
    "display(top_combos(long_df, [\"Schicht_label\",\"Station\",\"Ursache\"], n=20, sort_by=\"events\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6686882a",
   "metadata": {},
   "source": [
    "### 7.7 Zusammenhangsstärke zwischen Kategorien (Chi² / Cramér's V)\n",
    "\n",
    "Cramér's V gibt grob an, wie stark zwei kategoriale Variablen zusammenhängen (0 = kein Zusammenhang, 1 = sehr stark).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cramér's V (kategoriale Zusammenhänge) ---\n",
    "def cramers_v(confusion_matrix: pd.DataFrame) -> float:\n",
    "    obs = confusion_matrix.values.astype(float)\n",
    "    if obs.size == 0:\n",
    "        return np.nan\n",
    "    n = obs.sum()\n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "    row_sums = obs.sum(axis=1, keepdims=True)\n",
    "    col_sums = obs.sum(axis=0, keepdims=True)\n",
    "    expected = row_sums @ col_sums / n\n",
    "    mask = expected > 0\n",
    "    chi2 = ((obs[mask] - expected[mask])**2 / expected[mask]).sum()\n",
    "    r, k = obs.shape\n",
    "    phi2 = chi2 / n\n",
    "    denom = max(1e-9, min(k-1, r-1))\n",
    "    return float(np.sqrt(phi2 / denom))\n",
    "\n",
    "def show_cramers_v(var_a: str, var_b: str, top_b=None):\n",
    "    d = df[[var_a, var_b]].dropna().copy()\n",
    "    if top_b is not None:\n",
    "        top_vals = d[var_b].value_counts().head(top_b).index\n",
    "        d = d[d[var_b].isin(top_vals)]\n",
    "    cm = pd.crosstab(d[var_a], d[var_b])\n",
    "    v = cramers_v(cm)\n",
    "    print(f\"Cramér's V({var_a} vs {var_b}) =\", round(v, 3), \"| shape:\", cm.shape)\n",
    "    display(cm.head(20))\n",
    "\n",
    "show_cramers_v(\"Schicht_label\", \"Ursache\", top_b=12)\n",
    "show_cramers_v(\"Tagesblock\", \"Ursache\", top_b=12)\n",
    "show_cramers_v(\"Schicht_label\", \"MA_bin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46deb1f",
   "metadata": {},
   "source": [
    "### Bonus: Mini-Modell (optional) – welche Faktoren erklären Downtime am stärksten?\n",
    "\n",
    "Das ist kein „fertiges“ Predictive-Model, aber hilfreich, um schnell zu sehen, ob z.B. **Ursache** oder **Station** sehr dominant ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional: RandomForest zur Feature-Importance (wenn sklearn installiert ist) ---\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "    model_df = df.dropna(subset=[\"Downtime_min\",\"Start_hour\"]).copy()\n",
    "    model_df = model_df[model_df[\"Downtime_min\"] >= 0]\n",
    "\n",
    "    feature_cols_cat = [\"Schicht_label\",\"Station\",\"Ursache\",\"Tagesblock\",\"Wochentag_de\"]\n",
    "    feature_cols_num = [\"Start_hour\",\"MA\"]\n",
    "\n",
    "    X = model_df[feature_cols_cat + feature_cols_num].copy()\n",
    "    y = model_df[\"Downtime_min\"].astype(float)\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), feature_cols_cat),\n",
    "            (\"num\", \"passthrough\", feature_cols_num),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        min_samples_leaf=3\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    pipe = Pipeline(steps=[(\"pre\", pre), (\"rf\", rf)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    pred = pipe.predict(X_test)\n",
    "    print(\"MAE:\", round(mean_absolute_error(y_test, pred), 2), \"min\")\n",
    "    print(\"R²:\", round(r2_score(y_test, pred), 3))\n",
    "\n",
    "    ohe = pipe.named_steps[\"pre\"].named_transformers_[\"cat\"]\n",
    "    cat_names = ohe.get_feature_names_out(feature_cols_cat)\n",
    "    feature_names = np.concatenate([cat_names, np.array(feature_cols_num, dtype=object)])\n",
    "\n",
    "    importances = pipe.named_steps[\"rf\"].feature_importances_\n",
    "    imp = pd.DataFrame({\"feature\": feature_names, \"importance\": importances}).sort_values(\"importance\", ascending=False).head(25)\n",
    "    display(imp)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"sklearn nicht verfügbar oder Fehler beim Modelllauf:\", repr(e))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
